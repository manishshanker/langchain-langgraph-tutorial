{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Document Processing with LangChain\n",
    "\n",
    "In this tutorial, we'll explore document processing techniques using LangChain. We'll cover loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3208/4026724381.py:30: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65080a68f45949f187426f6bde9c54ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904ffa9d3d72440f9efcab3adabd8da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbf0e651c7644bab6c1d6e3f7e1d1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b28511e7c80452381ceeacab49b3341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba21a5ac7a94e1cb9f1180f885a2f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0905eb16c448e0bed17017141c7015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3220c4556a8847129e2f479c2f58e836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b4263650a14318a2cd61c7e422803e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16cd45c388846eab3eeb7f67e763556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dd33836221499ab6f873b7b87e341f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f2872dfe8249e9b5fd644a79b7db4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 768\n",
      "Embedding sample: [0.00037809900823049247, -0.050803475081920624, -0.03514723852276802, -0.023250970989465714, -0.044158272445201874]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm =  ChatGroq(\n",
    "        model_name=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )\n",
    "# print(os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "# embedding_model = OllamaEmbeddings(model=\"all-minilm\",base_url=os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "\n",
    "# Create the embedding model using Hugging Face Inference API\n",
    "# embedding_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "#     api_key=os.getenv(\"HF_API_KEY\"),\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "test_text = \"This is a test sentence.\"\n",
    "embedding = embedding_model.embed_query(test_text)\n",
    "print(\"Embedding length:\", len(embedding))\n",
    "print(\"Embedding sample:\", embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of sample1.txt:\n",
      "# Comprehensive Overview of Artificial Intelligence\n",
      "\n",
      "## Table of Contents\n",
      "1. [Introduction to Artificial Intelligence](#introduction-to-artificial-intelligence)\n",
      "2. [History of AI](#history-of-ai)\n",
      "3. [...\n",
      "\n",
      "Number of documents loaded: 1\n",
      "Document 1 preview: # Comprehensive Overview of Artificial Intelligenc...\n"
     ]
    }
   ],
   "source": [
    "# Load a single document\n",
    "loader = TextLoader(\"sample_documents/sample1.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "print(f\"Content of sample1.txt:\\n{document[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# Load multiple documents from a directory\n",
    "dir_loader = DirectoryLoader(\"sample_documents/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1} preview: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Carica il PDF\n",
    "loader = PyPDFLoader(\"sample_documents/sample2.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Splitting and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 110\n",
      "First split preview:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford University\n",
      "Georges Harik\n",
      "Notbad AI Inc\n",
      "Yijia Shao\n",
      "Stanford University\n",
      "Varuna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nic...\n"
     ]
    }
   ],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "print(f\"First split preview:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of these documents?\n",
      "Answer: The main topic of these documents appears to be a framework for learning to reason, specifically a method called Quiet-STaR, and its application to language models, including how it improves their ability to predict certain tokens and answer questions.\n",
      "\n",
      "Sources:\n",
      "Document 1: these tends to<|startthought|> in some sense - to be the more difficult<|\n",
      "endthought|> trickiest for...\n",
      "Document 2: A: I understand there is the ones day one 3 plus 4 eats who makes and so on and so on.\n",
      "M: But maybe ...\n",
      "Document 3: 5.2 Improvement Distribution\n",
      "As visualized in Appendix Figure 7, we find that on average there is li...\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(splits, embedding_model)\n",
    "\n",
    "# Create a retrieval-based QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the main topic of these documents?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Discuss the importance of AI\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "Result 1:\n",
      "these tends to<|startthought|> in some sense - to be the more difficult<|\n",
      "endthought|> trickiest for students\n",
      "Lastly, we include an example from answering CommonsenseQA. Notably, this thought\n",
      "occurs w...\n",
      "\n",
      "Result 2:\n",
      "Ethics Statement\n",
      "This work raises some important ethical questions, many of which also apply to STaR. For\n",
      "example, it is impossible to know that the reasoning expressed by the model in language\n",
      "accura...\n",
      "\n",
      "Result 3:\n",
      "process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS\n",
      "2022) Workshop on MATH-AI, 2022.\n",
      "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D\n",
      "Goodm...\n",
      "\n",
      "Question: What are some advantages of ai models?\n",
      "Answer: content='The provided context does not explicitly mention the advantages of AI models. However, it can be inferred that AI models, such as the one used in the context, have the ability to:\\n\\n1. **Process and analyze large amounts of data**: The context mentions a 7 billion parameter model, which suggests that AI models can handle complex and large datasets.\\n2. **Learn to reason and make decisions**: The context discusses a new framework for learning to reason, which implies that AI models can be trained to make decisions and reason like humans.\\n3. **Generate human-like language**: The context mentions that the model can express reasoning in language, which suggests that AI models can generate human-like language and communicate effectively.\\n4. **Improve language modeling**: The context mentions that the work aims to improve language modeling, which implies that AI models can be used to improve language understanding and generation.\\n\\nOverall, the advantages of AI models can be inferred to include their ability to process large amounts of data, learn to reason and make decisions, generate human-like language, and improve language modeling. However, the context also highlights some limitations and ethical concerns related to AI models, such as the potential for biased reasoning patterns and the lack of safeguards against harmful or biased reasoning.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 250, 'prompt_tokens': 624, 'total_tokens': 874, 'completion_time': 0.909090909, 'prompt_time': 0.14527733, 'queue_time': 0.10129690699999999, 'total_time': 1.054368239}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_34555b7c20', 'finish_reason': 'stop', 'logprobs': None} id='run-8b17a1a5-e4ee-48d0-adbf-2dfe3adaa04f-0' usage_metadata={'input_tokens': 624, 'output_tokens': 250, 'total_tokens': 874}\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search\n",
    "query = \"Discuss the importance of AI\"\n",
    "search_results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Search query: {query}\\n\")\n",
    "print(\"Top 3 relevant chunks:\")\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"Result {i+1}:\\n{doc.page_content[:200]}...\\n\")\n",
    "\n",
    "# Use the search results to answer a question\n",
    "question = \"What are some advantages of ai models?\"\n",
    "context = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "prompt = f\"Based on the following context, answer the question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of document processing with LangChain, including loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search. These techniques form the foundation for more advanced document analysis and information retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
