{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Document Processing with LangChain\n",
    "\n",
    "In this tutorial, we'll explore document processing techniques using LangChain. We'll cover loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 384\n",
      "Embedding sample: [0.08429646492004395, 0.057953670620918274, 0.00449336739256978, 0.10582108050584793, 0.00708338338881731]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm =  ChatGroq(\n",
    "        model_name=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )\n",
    "# print(os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "# embedding_model = OllamaEmbeddings(model=\"all-minilm\",base_url=os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "\n",
    "# Create the embedding model using Hugging Face Inference API\n",
    "# embedding_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "#     api_key=os.getenv(\"HF_API_KEY\"),\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "test_text = \"This is a test sentence.\"\n",
    "embedding = embedding_model.embed_query(test_text)\n",
    "print(\"Embedding length:\", len(embedding))\n",
    "print(\"Embedding sample:\", embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of sample1.txt:\n",
      "# Comprehensive Overview of Artificial Intelligence\n",
      "\n",
      "## Table of Contents\n",
      "1. [Introduction to Artificial Intelligence](#introduction-to-artificial-intelligence)\n",
      "2. [History of AI](#history-of-ai)\n",
      "3. [...\n",
      "\n",
      "Number of documents loaded: 1\n",
      "Document 1 preview: # Comprehensive Overview of Artificial Intelligenc...\n"
     ]
    }
   ],
   "source": [
    "# Load a single document\n",
    "loader = TextLoader(\"sample_documents/sample1.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "print(f\"Content of sample1.txt:\\n{document[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# Load multiple documents from a directory\n",
    "dir_loader = DirectoryLoader(\"sample_documents/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1} preview: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Carica il PDF\n",
    "loader = PyPDFLoader(\"sample_documents/sample2.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Splitting and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 110\n",
      "First split preview:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford University\n",
      "Georges Harik\n",
      "Notbad AI Inc\n",
      "Yijia Shao\n",
      "Stanford University\n",
      "Varuna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nic...\n"
     ]
    }
   ],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "print(f\"First split preview:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of these documents?\n",
      "Answer: The main topic of these documents appears to be the improvement of Language Models (LMs) through a technique called Quiet-STaR, which enables them to better reason and understand text, particularly in tasks that require commonsense reasoning and problem-solving.\n",
      "\n",
      "Sources:\n",
      "Document 1: improve the LMâ€™s ability to directly answer difficult questions. In particular,\n",
      "after continued pret...\n",
      "Document 2: these tends to<|startthought|> in some sense - to be the more difficult<|\n",
      "endthought|> trickiest for...\n",
      "Document 3: 5.2 Improvement Distribution\n",
      "As visualized in Appendix Figure 7, we find that on average there is li...\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(splits, embedding_model)\n",
    "\n",
    "# Create a retrieval-based QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the main topic of these documents?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Discuss the importance of AI\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "Result 1:\n",
      "process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS\n",
      "2022) Workshop on MATH-AI, 2022.\n",
      "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D\n",
      "Goodm...\n",
      "\n",
      "Result 2:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford University\n",
      "Georges Harik\n",
      "Notbad AI Inc\n",
      "Yijia Shao\n",
      "Stanford University\n",
      "Varuna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nic...\n",
      "\n",
      "Result 3:\n",
      "Proving. CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393. eprint:\n",
      "2009.03393.\n",
      "Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning\n",
      "emerges from the locality...\n",
      "\n",
      "Question: What are some advantages of ai models?\n",
      "Answer: content='Based on the provided context, some advantages of AI models include:\\n\\n1. **Ability to learn and infer unstated rationales**: AI models, such as language models, can learn to infer rationales from few-shot examples and even from arbitrary text, which can be useful in various applications.\\n2. **Improved reasoning capabilities**: AI models can be trained to perform reasoning tasks, such as chain-of-thought reasoning, which can lead to more accurate and informed decision-making.\\n3. **Ability to think before speaking**: AI models, like the Quiet-STaR model, can be designed to pause and think before generating text, which can lead to more coherent and well-reasoned responses.\\n4. **Inductive reasoning capabilities**: AI models can be trained to perform inductive reasoning, which involves making generalizations or drawing conclusions based on specific observations or data.\\n5. **Ability to learn from feedback**: AI models can learn from process-and outcome-based feedback, which can help improve their performance and adapt to new situations.\\n\\nThese advantages suggest that AI models have the potential to become more intelligent, flexible, and effective in various applications, such as natural language processing, decision-making, and problem-solving.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 240, 'prompt_tokens': 511, 'total_tokens': 751, 'completion_time': 0.872727273, 'prompt_time': 0.033180633, 'queue_time': 0.107689356, 'total_time': 0.905907906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None} id='run-3cd64b80-9049-4a4b-97f0-8d4b1d7d7c4b-0' usage_metadata={'input_tokens': 511, 'output_tokens': 240, 'total_tokens': 751}\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search\n",
    "query = \"Discuss the importance of AI\"\n",
    "search_results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Search query: {query}\\n\")\n",
    "print(\"Top 3 relevant chunks:\")\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"Result {i+1}:\\n{doc.page_content[:200]}...\\n\")\n",
    "\n",
    "# Use the search results to answer a question\n",
    "question = \"What are some advantages of ai models?\"\n",
    "context = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "prompt = f\"Based on the following context, answer the question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of document processing with LangChain, including loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search. These techniques form the foundation for more advanced document analysis and information retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
