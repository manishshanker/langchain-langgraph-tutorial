{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Introduction to LangChain\n",
    "\n",
    "Welcome to our first tutorial on LangChain! In this notebook, we'll cover the basics of LangChain and create a simple application using the Groq LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is LangChain?\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex LLM-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation and Setup\n",
    "\n",
    "First, let's make sure we have the necessary packages installed:\n",
    "<br> using the command pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the required modules and set up our Groq API key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature and Top_p Sampling\n",
    "\n",
    "1. Temperature:\n",
    "    - Controls the randomness/creativity of text generation\n",
    "    - Range: 0.0 to 1.0 (can go higher, but not recommended)\n",
    "    - Lower values (e.g., 0.2): More focused, deterministic output\n",
    "    - Higher values (e.g., 0.7): More diverse, creative output\n",
    "    - Temperature 0: Always selects the most probable next token\n",
    "    - Affects the probability distribution of possible tokens\n",
    "\n",
    "2. Top_p Sampling (Nucleus Sampling):\n",
    "    - Alternative to temperature sampling\n",
    "    - Range: 0.0 to 1.0\n",
    "    - Considers only the subset of tokens whose cumulative probability mass reaches the top_p threshold\n",
    "    - Example: top_p 0.1 considers only the top 10% most likely tokens\n",
    "    - Allows for dynamic vocabulary selection based on context\n",
    "\n",
    "3. Usage:\n",
    "    - Can be used independently or together\n",
    "    - Adjust based on desired level of creativity and control\n",
    "\n",
    "4. Extended Concepts:\n",
    "    - Perplexity: Measure of how well the model predicts the text. Lower perplexity indicates better prediction.\n",
    "    - Beam Search: Technique to find the most likely sequence of tokens by exploring multiple possibilities simultaneously.\n",
    "    - Length Penalty: Encourages or discourages the model to generate longer or shorter sequences.\n",
    "\n",
    "5. Best Practices:\n",
    "    - Experiment with different values for your specific use case\n",
    "    - Start with middle values (e.g., temperature 0.5, top_p 0.5) and adjust as needed\n",
    "    - For factual or technical tasks, use lower temperature and top_p\n",
    "    - For creative tasks, use higher temperature and top_p\n",
    "    - Consider using top_p sampling for more consistent results across different prompts\n",
    "\n",
    "6. Example Use Cases and Recommended Values:\n",
    "    Use Case                 | Temperature | Top_p | Description\n",
    "    ------------------------ | ----------- | ----- | -----------\n",
    "    Code Generation          | 0.2         | 0.1   | Focused, convention-adhering code\n",
    "    Creative Writing         | 0.7         | 0.8   | Diverse, exploratory text\n",
    "    Chatbot Responses        | 0.5         | 0.5   | Balanced coherence and diversity\n",
    "    Code Comment Generation  | 0.3         | 0.2   | Concise, relevant comments\n",
    "    Data Analysis Scripting  | 0.2         | 0.1   | Correct, efficient scripts\n",
    "    Exploratory Code Writing | 0.6         | 0.7   | Creative, alternative solutions\n",
    "\n",
    "7. Advanced Techniques:\n",
    "    - Fine-tuning: Adapt the model to specific tasks or domains for better performance\n",
    "    - Prompt Engineering: Craft effective prompts to guide the model's output\n",
    "    - Few-shot Learning: Provide examples in the prompt to steer the model's behavior\n",
    "\n",
    "8. Ethical Considerations:\n",
    "    - Be aware of potential biases in the model's output\n",
    "    - Implement content filtering for sensitive applications\n",
    "    - Respect copyright and intellectual property rights\n",
    "\n",
    "9. Performance Optimization:\n",
    "    - Use caching mechanisms to store frequently requested outputs\n",
    "    - Implement request batching for multiple related queries\n",
    "    - Consider using smaller models for faster response times in less complex tasks\n",
    "\n",
    "Remember to refer to the official OpenAI documentation for the most up-to-date information and best practices when working with the GPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.5, \"seed\": 1337}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Concepts: Chains, Agents, and Memory\n",
    "\n",
    "LangChain introduces several key concepts:\n",
    "\n",
    "- **Chains**: Sequences of operations to be performed with an LLM\n",
    "- **Agents**: Autonomous entities that can use tools and make decisions\n",
    "- **Memory**: Systems for storing and retrieving information across interactions\n",
    "\n",
    "In this tutorial, we'll focus on creating a simple chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First LangChain Application\n",
    "\n",
    "Let's create a simple application that generates a short story based on a given prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Simple LangChain LLM Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "#Utilize the LLMChain to generate a simple response\n",
    "resp=llm.invoke(\"Hello, how are you?\")\n",
    "#Print only the text of the response\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"As a time traveler, Emma had one rule: observe, don't interact. But when she witnessed a young Leonardo da Vinci struggling to fix his broken flying machine, she couldn't resist lending a hand. With her modern tools, the repair was effortless. However, her actions had an unforeseen consequence: da Vinci's invention soared to success decades earlier than it should have. Upon returning to her own time, Emma discovered a world where the Renaissance had accelerated, and the course of history had been forever altered. The once-familiar world was now unrecognizable, and Emma's mistake had changed everything.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 68, 'total_tokens': 190, 'completion_time': 0.522560187, 'prompt_time': 0.003770394, 'queue_time': 0.100763266, 'total_time': 0.526330581}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_90c1d253ff', 'finish_reason': 'stop', 'logprobs': None} id='run-28257eb5-3991-4bf7-afcc-1ff03ce5c782-0' usage_metadata={'input_tokens': 68, 'output_tokens': 122, 'total_tokens': 190}\n",
      "------------------------------------------------------\n",
      "As a time traveler, Emma had one rule: observe, don't interact. But when she witnessed a young Leonardo da Vinci struggling to fix his broken flying machine, she couldn't resist lending a hand. With her modern tools, the repair was effortless. However, her actions had an unforeseen consequence: da Vinci's invention soared to success decades earlier than it should have. Upon returning to her own time, Emma discovered a world where the Renaissance had accelerated, and the course of history had been forever altered. The once-familiar world was now unrecognizable, and Emma's mistake had changed everything.\n",
      "Total Tokens: 190\n",
      "Input Tokens: 68\n",
      "Output Tokens: 122\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "You are a creative writer. Write a short story (about 100 words) based on the following prompt:\n",
    "\n",
    "Prompt: {prompt}\n",
    "\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
    "\n",
    "# Create a chain\n",
    "story_chain = prompt | llm\n",
    "\n",
    "# Generate a story\n",
    "story_prompt = \"A time traveler accidentally changes history\"\n",
    "result = story_chain.invoke(story_prompt)\n",
    "\n",
    "#All the text of the response\n",
    "print(result)\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "#Only the text of the response\n",
    "print(result.content)\n",
    "\n",
    "#Token Used\n",
    "print(\"Total Tokens: \"+ result.response_metadata[\"token_usage\"][\"total_tokens\"].__str__())\n",
    "print(\"Input Tokens: \"+result.response_metadata[\"token_usage\"][\"prompt_tokens\"].__str__())\n",
    "print(\"Output Tokens: \"+ result.response_metadata[\"token_usage\"][\"completion_tokens\"].__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just created your first LangChain application. This simple example demonstrates how to use a prompt template, create a chain, and generate content using an LLM.\n",
    "\n",
    "In the next tutorial, we'll explore more advanced features of LangChain and dive deeper into working with language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
